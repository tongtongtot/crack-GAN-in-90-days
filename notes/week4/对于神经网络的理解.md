要理解神经网络中“黑箱”的问题，我认为首先要先认识一下神经网络本身。

神经网络其实没有很多人说的那么神秘，简单来说，神经网络就是把我们有的转换成我们要的，仅此而已，具体的操作更像是把一些数据输入到计算器中然后得到一个结果。

用算式表示的话，就是 F(x) = y：x为神经网络的输入，y为神经网络的输出，而 F() 则是神经网络本身。

x 和 y 可以是任意的东西，可以是 视频、图像、文字、数字、序列 以及各种各样能输入到电脑中保存为二进制的数据。

如果用 “天气预报系统” 举例的话就是： 输入之前一段时间的天气数据，输出之后可能的天气数据。在这里，之前一段时间的天气数据就是 x, 之后可能的天气数据就是 y. 

到这里，你可能会觉得说这和之前的统计学方法相差无几，因为统计学方法很可能也是将一个 x 输入一个算式 G(), 然后得出一个y （个人理解，可能有误）; 但是，我认为这就是统计学方法和神经网络的最大差异。统计学方法的G(x) 很可能是不变的，是一个确定的算式，但是神经网络训练出的 F(x) 则是根据不同情况变化的。

简单来说，F(x) = ax + b, G(x) = c1x +c2; c 为常数，而 a,b 则为变量，通过梯度下降算法减少 F(x) 与真实值的差可以找到一组最优的 a 和 b 使得 F(x) 比 G(x) 更加接近真实的y.

现在，问题来了，c1 和 c2 是通过现有的理论推出来的，但 a 和 b 不是，而且很可能同时存在很多组 (a,b) 都能使得 F(x) 比 G(x) 更接近于真实的 y. 但为什么这样的 (a,b) 会比 (c1,c2) 要好是无法解释的。

比如，当 x = 1, y = 4 时，

 F(x) = 3x + 1 和 F(x) = 2x + 2 都可以做到 F(x) = y, 这给解释的工作带来了大量的麻烦，因为有些理论可能适用于 F(x) = 3x + 1, 但不适用于 F(x) = 2x+2, 甚至有时候模型还可能因为各种各样的原因出现 F(x) = 10000x - 9996 的情况。

因此我们说神经网络是一个黑箱。

当然，神经网络还有另一个问题，就是我们不知道它是基于什么判断的。

还是回到天气预报的例子，正常情况下，我们给模型输入一个 x, 便只能得到一个 y, 至于 x 是如何得到 y 的，我们无从入手。

这时候有人可能会说：我们不是知道 F(x) 吗，分析 F(x) 不就行了吗？ 

其实除了因为上文所述难以分析 F(x) 之外，结果也不直观。比如我们不知道模型具体是通过哪一篇云或者那一部分气象数据得知明天要下雨的。

但在这方面已经有一些文章开始着手解决这样的问题了。这便是模型可视化。

比如大名鼎鼎的 grad-cam 就能找出模型的 “重心” 在什么位置，可以高亮模型使用数据的重心是什么，从某种角度来说为这个黑箱开了一个小洞，照进了一束光。